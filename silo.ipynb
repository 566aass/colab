{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhnHayQ/7ydKkX1v6Q95Ec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/566aass/colab/blob/colab_code/silo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "# 挂载 Google 云盘\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysj6_W4RVGRa",
        "outputId": "b5d15a35-baeb-453c-91e7-da5d527c3ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.3.0  # 安装 scikit-learn，包含 train_test_split\n",
        "!pip install tensorflow==2.13.0     # 安装 TensorFlow，keras 的后端\n",
        "!pip install keras==2.13.1       # 安装 Keras\n",
        "!pip install numpy==1.25.2        # 安装 NumPy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4hRC4samRFr_",
        "outputId": "970450a0-88a4-467e-a706-c89a90b91f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.3.0\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (3.5.0)\n",
            "Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.3.0\n",
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.68.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.12.1)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (18.1.1)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.5.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.36 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "langchain-core 0.3.19 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.54.4 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.9.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.23.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.13.0 which is incompatible.\n",
            "torch 2.5.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.4.1 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ed57f008b8c949f29d5acdcbe9446693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.25.2 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "cc4b9dcbf64445838e081f456faa3e19"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LeakyReLU,Dropout,ReLU\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "# 数据预处理\n",
        "def preprocess_data(data):\n",
        "    dataset = data\n",
        "\n",
        "    # 将数据划分为训练集和测试集\n",
        "    # 注意：这里假设目标变量是每个样本对应的最大降雨侵蚀月份（0-11）\n",
        "    scaler = MinMaxScaler()\n",
        "    X = scaler.fit_transform(dataset)\n",
        "    y = np.argmax(dataset, axis=1)  # 目标变量，表示最大降雨侵蚀月份（0-11）\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, input_shape=input_shape))\n",
        "    model.add(ReLU())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32))\n",
        "    model.add(ReLU())\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = SGD(learning_rate=0.01, momentum=0.9, decay=1e-6, nesterov=True)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'accuracy'])\n",
        "    # model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# 读取数据文件，并且筛选数据，然后转换数据维度\n",
        "df = pd.read_csv('/content/drive/MyDrive/Neu/silo.csv')\n",
        "\n",
        "# 筛选数据\n",
        "data = df.iloc[:, 3:].fillna(0).values  # 使用 0 填充缺失值\n",
        "\n",
        "# 拆分数据\n",
        "x_train, x_test, y_train, y_test = preprocess_data(data)\n",
        "y_train_onehot = to_categorical(y_train, num_classes=12)\n",
        "y_test_onehot = to_categorical(y_test, num_classes=12)\n",
        "\n",
        "# 创建模型\n",
        "input_shape = (x_train.shape[1], 1)\n",
        "model = create_model(input_shape)\n",
        "\n",
        "# 进行模型训练\n",
        "model.fit(x_train, y_train_onehot, epochs=10, batch_size=64, validation_split=0.2)\n",
        "loss, mae, accuracy = model.evaluate(x_test, y_test_onehot, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test MAE: {mae:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 获取最后一层神经网络的权重，其形状为(12, 12)\n",
        "# weights = model.layers[-1].get_weights()[0]\n",
        "# # 将权重转换为一维数组\n",
        "# weights_array = weights.flatten()\n",
        "# print(weights_array)\n",
        "# # 获取权重绝对值最大的索引\n",
        "# label_index = np.argmax(np.abs(weights_array))\n",
        "# # 根据索引值获取对应的月份\n",
        "# max_weight_index = label_index % 12 + 1\n",
        "# # 构建标签与月份的映射关系\n",
        "# month_mapping = {1: '一月', 2: '二月', 3: '三月', 4: '四月', 5: '五月', 6: '六月',\n",
        "#                  7: '七月', 8: '八月', 9: '九月', 10: '十月', 11: '十一月', 12: '十二月'}\n",
        "# # 根据索引值获取对应的月份\n",
        "# target_month = month_mapping[max_weight_index]\n",
        "\n",
        "# print('降雨侵蚀最大的月份：', target_month)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJzllIMgI7gk",
        "outputId": "846540aa-9a7d-49c3-bce7-b670158a0a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9167 - loss: 0.0788 - mae: 0.1422 - val_accuracy: 0.9167 - val_loss: 0.0765 - val_mae: 0.1542\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.0768 - mae: 0.1518 - val_accuracy: 0.9167 - val_loss: 0.0765 - val_mae: 0.1542\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.0767 - mae: 0.1525 - val_accuracy: 0.9167 - val_loss: 0.0765 - val_mae: 0.1545\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.0766 - mae: 0.1525 - val_accuracy: 0.9167 - val_loss: 0.0765 - val_mae: 0.1558\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9167 - loss: 0.0766 - mae: 0.1528 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1548\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.0766 - mae: 0.1525 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1554\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.0765 - mae: 0.1528 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1549\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9167 - loss: 0.0765 - mae: 0.1529 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1550\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.0765 - mae: 0.1528 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1547\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9167 - loss: 0.0765 - mae: 0.1526 - val_accuracy: 0.9167 - val_loss: 0.0764 - val_mae: 0.1545\n",
            "Test Loss: 0.0764\n",
            "Test MAE: 0.1543\n",
            "Test Accuracy: 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# 加载数据\n",
        "df = pd.read_csv('/content/drive/MyDrive/Neu/silo.csv')\n",
        "\n",
        "# 筛选数据并填充缺失值\n",
        "data = df.iloc[:, 3:].fillna(0).values\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # 特征缩放\n",
        "    scaler = MinMaxScaler()\n",
        "    X = scaler.fit_transform(data)\n",
        "\n",
        "    # 目标变量：降雨侵蚀最大的月份索引 (0-11)\n",
        "    y = np.argmax(data, axis=1)\n",
        "\n",
        "    # 划分训练集和测试集\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 将训练集和测试集转换为 RNN 模型所需的形状 (samples, timesteps, features)\n",
        "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=64, input_shape=input_shape))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
        "    return model\n",
        "# 设置随机种子\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 数据预处理\n",
        "x_train, x_test, y_train, y_test = preprocess_data(data)\n",
        "\n",
        "# 创建模型\n",
        "input_shape = (x_train.shape[1], 1)  # (12, 1)\n",
        "print(input_shape)\n",
        "model = create_rnn_model(input_shape)\n",
        "\n",
        "# 训练模型\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# 预测\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# 将预测结果转换为月份 (0-11 -> 1-12)\n",
        "predicted_months = [int(pred[0]) + 1 for pred in predictions]\n",
        "predicted_labels = [month - 1 for month in predicted_months]\n",
        "\n",
        "# 计算准确率\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "\n",
        "# 计算精确率 (macro average)\n",
        "precision = precision_score(y_test, predicted_labels, average='macro')\n",
        "\n",
        "# 计算召回率 (macro average)\n",
        "recall = recall_score(y_test, predicted_labels, average='macro')\n",
        "\n",
        "# 计算 F1 分数 (macro average)\n",
        "f1 = f1_score(y_test, predicted_labels, average='macro')\n",
        "\n",
        "# 打印指标\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# 计算混淆矩阵\n",
        "confusion = confusion_matrix(y_test, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA1XZGDEpzPT",
        "outputId": "717e017b-ecdd-4105-9753-ca921efbcd9d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 1)\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - loss: 37.0706 - mae: 4.6841 - val_loss: 26.3703 - val_mae: 4.9548\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 25.6177 - mae: 4.9032 - val_loss: 26.3054 - val_mae: 4.9448\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 25.5546 - mae: 4.8979 - val_loss: 26.1766 - val_mae: 4.9462\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 25.3324 - mae: 4.8817 - val_loss: 23.6512 - val_mae: 4.6120\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 23.5326 - mae: 4.5714 - val_loss: 23.7026 - val_mae: 4.3862\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 22.7344 - mae: 4.3445 - val_loss: 22.6076 - val_mae: 4.3666\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 22.3295 - mae: 4.3616 - val_loss: 22.5681 - val_mae: 4.1900\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 22.1210 - mae: 4.3212 - val_loss: 21.9408 - val_mae: 4.1949\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 21.6849 - mae: 4.2522 - val_loss: 22.0113 - val_mae: 4.2689\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 21.1934 - mae: 4.2196 - val_loss: 21.2300 - val_mae: 4.1510\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 20.7321 - mae: 4.1359 - val_loss: 20.6132 - val_mae: 4.1203\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 20.1343 - mae: 4.0447 - val_loss: 19.6358 - val_mae: 4.0295\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 19.1790 - mae: 3.8781 - val_loss: 19.0512 - val_mae: 3.9125\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 18.0296 - mae: 3.6686 - val_loss: 17.8074 - val_mae: 3.5364\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 17.2356 - mae: 3.4428 - val_loss: 18.0058 - val_mae: 3.6124\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 17.0129 - mae: 3.4283 - val_loss: 17.6583 - val_mae: 3.4874\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16.7380 - mae: 3.3698 - val_loss: 17.6112 - val_mae: 3.4237\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16.5570 - mae: 3.3187 - val_loss: 17.5453 - val_mae: 3.3858\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 16.3957 - mae: 3.2827 - val_loss: 17.4676 - val_mae: 3.3579\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 16.2564 - mae: 3.2551 - val_loss: 17.3981 - val_mae: 3.3371\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "Accuracy: 0.127\n",
            "Precision: 0.10895390867607264\n",
            "Recall: 0.05985938411470326\n",
            "F1 Score: 0.05198580661044832\n",
            "Confusion Matrix:\n",
            "[[ 39 141  19  27  18  16   8   5   2   0   0   0]\n",
            " [  3  80  88  55  26  22  18  16  15   4   2   0]\n",
            " [  0   9   8   3   2   2   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   1   1   0   0   0   0]\n",
            " [  0   0   0   0   1   2   2   0   0   1   0   0]\n",
            " [  0  21  18  42  39  37  43  48  50  44  22   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score # 导入cohen_kappa_score\n",
        "\n",
        "\n",
        "\n",
        "# 加载数据\n",
        "df = pd.read_csv('/content/drive/MyDrive/Neu/silo.csv')\n",
        "\n",
        "# 筛选数据并填充缺失值\n",
        "data = df.iloc[:, 3:].fillna(0).values\n",
        "\n",
        "# 目标变量：降雨侵蚀最大的月份索引 (0-11)\n",
        "target = np.argmax(data, axis=1)\n",
        "\n",
        "# 划分训练集和测试集\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "print(np.unique(target))\n",
        "# 创建随机森林分类器\n",
        "model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
        "\n",
        "# 训练模型\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# 预测\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# 计算准确率\n",
        "overall_accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "# 计算 Kappa 系数\n",
        "kappa = cohen_kappa_score(y_test, predictions)\n",
        "\n",
        "# 计算混淆矩阵\n",
        "confusion_mat = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# 打印指标\n",
        "print(f\"总体精度: {overall_accuracy}\")\n",
        "print(f\"Kappa 系数: {kappa}\")\n",
        "print(f\"混淆矩阵:\\n{confusion_mat}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPxhVJjyKWpT",
        "outputId": "50f75fcc-86d0-4abf-858a-895fd9f03509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  9 10 11]\n",
            "总体精度: 0.943\n",
            "Kappa 系数: 0.9163203257928649\n",
            "混淆矩阵:\n",
            "[[252  11   0   0   1  11]\n",
            " [  3 320   0   0   0   6]\n",
            " [  0   2  22   0   0   0]\n",
            " [  0   0   0   0   0   2]\n",
            " [  1   1   0   1   3   0]\n",
            " [  9   8   1   0   0 346]]\n"
          ]
        }
      ]
    }
  ]
}